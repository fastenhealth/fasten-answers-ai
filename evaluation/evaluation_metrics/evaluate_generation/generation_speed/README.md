# Fasten Answers AI - Generation Speed Evaluation

This project is designed to evaluate the response generation speed of various language models, allowing for performance comparison across different parameters. The project currently supports Llama 3.1 and Phi 3.5 models, which are executed via `llama.cpp` using Docker.

## 1. Requirements

- Docker installed and configured
- Python 3.9+
- Dependencies listed in `./fasten-answers-ai/requirements.txt`

## 2. Project Structure

- `main.py`: Main script that loads configuration parameters, create contexts and questions, and generates responses.
- `contexts_and_questions.py`: Contains logic to create contexts and questions based on templates.
- `generator.py`: Generates model responses and stores them in a CSV file.
- `settings.py`: System configuration, such as prompts, templates, and supported models.
- `data/input.json`: Configuration file where model parameters are specified, including the host, model, number of cores, temperature, etc.

## 3. Installation

1. Clone the repository:

    ```bash
    git clone https://github.com/fastenhealth/fasten-answers-ai.git
    cd fasten-answers-ai/scripts
    ```

2. Create a virtual environment and install dependencies:

    ```bash
    python -m venv venv
    source venv/bin/activate
    pip install -r requirements.txt
    ```

3. Ensure you have the models in the `fasten-answers-ai/models` folder in .gguf format. Supported models are:
   - `meta-llama/Meta-Llama-3.1-Instruct`
   - `microsoft/Phi-3.5-mini-instruct`

## 4. Configuration

1. You must modify the values in the `evaluation/evaluation_metrics/evaluate_generation/generation_speed/data/input.json` file to configure the model, number of cores, temperature, and other parameters. As example:

```json
{
    "host": "http://localhost:8090",
    "model": "microsoft/Phi-3.5-mini-instruct",
    "output_file": "Phi-3.5-mini-instruct-F16.csv",
    "total_cores": 10,
    "temperature": 0.8,
    "tokens_to_predict": 200
}
```

### Parameters Explanation

* host: the address of the server running the model.
* model: name of the model to use. Currently configured values are `meta-llama/Meta-Llama-3.1-Instruct` and `microsoft/Phi-3.5-mini-instruct`
* output_file: name of the CSV file where the results will be saved.
* total_cores: Number of cores to use for generation.
* temperature: Controls the randomness in text generation (0.8 is a good default value).
* tokens_to_predict: Number of tokens to predict in the generation.

## 5. Running the model

1. Run llama.cpp, using docker. In this example we use Phi-3.5-mini-instruct-F16, but feel free to change the model if required:

```bash
docker run --rm -p 9090:9090 -v $(pwd)/models:/models ghcr.io/ggerganov/llama.cpp:server -m  models/Phi-3.5-mini-instruct-F16.gguf -t 10 -n 400 -c 2048 --host 0.0.0.0 --port 9090
```

### Command Explanation

    -p 9090:9090: Exposes port 9090 of the container on port 9090 of the host.
    -v $(pwd)/models:/models: Mounts the models folder to the container.
    -m models/Phi-3.5-mini-instruct-F16.gguf: Specifies the model to use.
    -t 10: Number of cores to use.
    -n 400: Number of tokens to predict.
    -c 2048: Context size.

## 6. Running Elastic Search container

While Elasticsearch is not required for evaluating generation speed, since we're using the llama.cpp client declared in the main app (which imports Elasticsearch), we need to run the container to avoid errors.

```bash
docker run --rm -p 9200:9200 \                    
  -e "discovery.type=single-node" \
  -e "xpack.security.enabled=false" \
  -e "xpack.security.http.ssl.enabled=false" \
  --memory="512m" \
  -v data:/usr/share/elasticsearch/data \
  --name elasticsearch \
  docker.elastic.co/elasticsearch/elasticsearch:8.12.1
```

## 7. Running the Main Script

Once the container is running and `input.json` is configured, you can run the `main.py` script:

```bash
python -m evaluation.evaluation_metrics.evaluate_generation.generation_speed.main
```
    
This script will generate responses based on the defined contexts and questions, and save the results in a CSV file located in the data_speed folder in the root of the repository.
    
### Results

The generated results will be stored in data_speed folder in the root of the repository. The CSV will contain the following columns:

    model: Name of the model used.
    context_size: Size of the context in tokens.
    total_cores: Number of cores used.
    prompt: Context used.
    question: Question asked.
    response: Response generated by the model.
    temperature: Temperature value used.
    n_predict: Number of tokens predicted.
    tokens_predicted, tokens_evaluated: Metrics about the generated tokens.
    timings: Detailed generation timings, including:
        prompt_n, prompt_ms, prompt_per_token_ms, prompt_per_second
        predicted_n, predicted_ms, predicted_per_token_ms, predicted_per_second


