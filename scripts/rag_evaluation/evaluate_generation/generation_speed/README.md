# Fasten Answers AI - Generation Speed Evaluation

This project is designed to evaluate the response generation speed of various language models, allowing for performance comparison across different parameters. The project currently supports Llama 3.1 and Phi 3.5 models, which are executed via `llama.cpp` using Docker.

## 1. Requirements

- Docker installed and configured
- Python 3.9+
- Dependencies listed in `./fasten-answers-ai/scripts/requirements-dev.txt`

## 2. Project Structure

- `main.py`: Main script that loads configuration parameters, configures the LLM client, and generates responses.
- `contexts_and_questions.py`: Contains logic to create contexts and questions based on templates.
- `generator.py`: Generates model responses and stores them in a CSV file.
- `llm_client.py`: Configures the LLM client to interact with the models.
- `settings.py`: System configuration, such as prompts, templates, and supported models.
- `data/input.json`: Configuration file where model parameters are specified, including the host, model, number of cores, temperature, etc.

## 3. Installation

1. Clone the repository:

    ```bash
    git clone https://github.com/fastenhealth/fasten-answers-ai.git
    cd fasten-answers-ai/scripts
    ```

2. Create a virtual environment and install dependencies:

    ```bash
    python -m venv venv-dev
    source venv-dev/bin/activate  # On Windows use venv-dev\Scripts\activate
    pip install -r requirements-dev.txt
    ```

3. Ensure you have the models in the `fasten-answers-ai/models` folder in .gguf format. Supported models are:
   - `meta-llama/Meta-Llama-3.1-Instruct`
   - `microsoft/Phi-3.5-mini-instruct`

## 4. Configuration

1. Go to generation speed evaluation folder

    ```bash
    cd rag_evaluation/evaluate_generation/generation_speed
    ```
2. You must modify the values in the `data/input.json` file to configure the model, number of cores, temperature, and other parameters. As example:

```json
{
    "host": "http://localhost:8090",
    "model": "microsoft/Phi-3.5-mini-instruct",
    "output_file": "Phi-3.5-mini-instruct-F16.csv",
    "total_cores": 10,
    "temperature": 0.8,
    "tokens_to_predict": 200
}
```

### Parameters Explanation

* host: the address of the server running the model.
* model: name of the model to use. Currently configured values are `meta-llama/Meta-Llama-3.1-Instruct` and `microsoft/Phi-3.5-mini-instruct`
* output_file: name of the CSV file where the results will be saved.
* total_cores: Number of cores to use for generation.
* temperature: Controls the randomness in text generation (0.8 is a good default value).
* tokens_to_predict: Number of tokens to predict in the generation.

## 5. Running the model

To run a model, ensure the Docker container with llama.cpp is active. Below is an example of running the Phi 3.5 mini model in Docker using llama.cpp from the root directory of the repository:

```bash
docker run --rm -p 8090:9090 -v $(pwd)/models:/models ghcr.io/ggerganov/llama.cpp:server -m  models/Phi-3.5-mini-instruct-F16.gguf -t 10 -n 400 -c 2048 --host 0.0.0.0 --port 9090
```

### Command Explanation

    -p 8090:9090: Exposes port 9090 of the container on port 8090 of the host.
    -v $(pwd)/models:/models: Mounts the models folder to the container.
    -m models/Phi-3.5-mini-instruct-F16.gguf: Specifies the model to use.
    -t 10: Number of cores to use.
    -n 400: Number of tokens to predict.
    -c 2048: Context size.

## 6. Running the Main Script

Once the container is running and `input.json` is configured, you can run the `main.py` script:

```bash
python main.py
```
    
This script will generate responses based on the defined contexts and questions, and save the results in a CSV file located in the data folder.
    
### Results

The generated results will be stored in the file specified in the output_file field in input.json. The CSV will contain the following columns:

    model: Name of the model used.
    context_size: Size of the context in tokens.
    total_cores: Number of cores used.
    prompt: Context used.
    question: Question asked.
    response: Response generated by the model.
    temperature: Temperature value used.
    n_predict: Number of tokens predicted.
    tokens_predicted, tokens_evaluated: Metrics about the generated tokens.
    timings: Detailed generation timings, including:
        prompt_n, prompt_ms, prompt_per_token_ms, prompt_per_second
        predicted_n, predicted_ms, predicted_per_token_ms, predicted_per_second


