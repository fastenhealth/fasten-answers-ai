import argparse
import json
import random
import requests

from clearml import Task


def calculate_retrieval_metrics(openai_responses,
                                num_sampled_questions,
                                endpoint_url,
                                k=5):
    # Initialize counters and sums for metrics
    total_questions = 0
    total_contexts_found = 0
    position_sum = 0
    reciprocal_rank_sum = 0
    precision_sum = 0
    recall_sum = 0
    openai_json_error = 0

    # Iterate over the OpenAI responses
    for response_id, response in enumerate(openai_responses):
        custom_id = response["custom_id"]

        content = response["response"]["body"]["choices"][0]["message"]["content"]

        try:
            questions_and_answers = json.loads(content)["questions_and_answers"]
        except json.JSONDecodeError as e:
            openai_json_error = +1
            if openai_json_error % 100 == 0:
                print(response_id)
            continue  # Skip to the next response if there's an error

        context = entry_dict.get(custom_id)

        if len(questions_and_answers) > 0:
            # Sample questions to measure
            if num_sampled_questions and len(questions_and_answers) > 0:
                questions_and_answers = [random.choice(questions_and_answers)]

            if context:
                context_str = context["resource"]

                for qa in questions_and_answers:
                    if isinstance(qa, dict) and "question" in qa:
                        if response_id % 100 == 0:
                            print(f"Response # {response_id}")

                        question = qa["question"]
                        total_questions += 1

                        # Query the search endpoint
                        params = {
                            'query': question,
                            'k': k
                        }
                        response = requests.get(endpoint_url, params=params)
                        search_results = response.json()

                        # Calculate metrics for each question
                        found = False
                        rank = 0

                        for i, result in enumerate(search_results):
                            if context_str in result["content"]:
                                if not found:
                                    total_contexts_found += 1
                                    rank = i + 1
                                    reciprocal_rank_sum += 1 / rank
                                    found = True

                                break  # Stop after finding the first relevant document

                        if found:
                            position_sum += rank
                            precision_sum += 1 / len(search_results)
                            recall_sum += 1  # Since there's only one relevant document

    # Calculate final metrics
    retrieval_accuracy = total_contexts_found / total_questions if total_questions > 0 else 0
    average_position = position_sum / total_contexts_found if total_questions > 0 else 0
    mrr = reciprocal_rank_sum / total_questions if total_questions > 0 else 0
    average_precision = precision_sum / total_questions if total_questions > 0 else 0
    average_recall = recall_sum / total_questions if total_questions > 0 else 0

    return {
        "retrieval_accuracy": retrieval_accuracy,
        "average_position": average_position,
        "mrr": mrr,
        "average_precision": average_precision,
        "average_recall": average_recall,
        "total_questions": total_questions,
        "total_openai_json_errors": openai_json_error,
        "total_contexts_found": total_contexts_found,
        "position_sum": position_sum
    }


if __name__ == "__main__":
    # Define input parameters
    parser = argparse.ArgumentParser(description="Evaluate retrieval metrics")

    parser.add_argument('experiment_iteration',
                        type=int,
                        help='The iteration number of the current experiment in a series.')
    parser.add_argument('num_sampled_questions',
                        type=int,
                        help='The number of questions to sample from the set of questions generated by OpenAI.')
    parser.add_argument('endpoint_url',
                        type=str,
                        default="http://0.0.0.0:8000/search",
                        help='The URL with the endpoint to query for search results.')

    args = parser.parse_args()

    experiment_iteration = args.experiment_iteration
    num_sampled_questions = args.num_sampled_questions
    endpoint_url = args.endpoint_url

    # Initialize clearml task and endpoint URL
    task = Task.init(project_name="Fasten",
                     task_name=f"Retrieval evaluation {experiment_iteration}")

    # Load JSON data
    with open('../data/json_sample_parsed.json', 'r') as f:
        parsed_data = json.load(f)

    entry_dict = {str(entry["node_id"]): entry for entry in parsed_data["entry"]}

    openai_responses = []
    with open('../data/batch_3haP1i8Dsfohov5umgZanshX_output.jsonl', 'r') as f:
        for line in f:
            openai_responses.append(json.loads(line))

    # Initialize counters
    total_contexts = len(entry_dict)
    total_questions = 0
    total_contexts_found = 0
    position_sum = 0
    k = 5

    # Evaluate metrics
    metrics = calculate_retrieval_metrics(openai_responses,
                                          num_sampled_questions,
                                          endpoint_url)

    metrics["Total contexts"] = total_contexts

    for series_name, value in metrics.items():
        task.get_logger().report_scalar(
            title="Retrieval Evaluation metric",
            series=series_name,
            value=value,
            iteration=0
        )

    task.close()
