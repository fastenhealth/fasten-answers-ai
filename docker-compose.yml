services:
  app:
    build:
      context: .
      dockerfile: Dockerfile
    environment:
      ES_HOST: http://elasticsearch:9200
      ES_USER: elastic
      ES_PASSWORD: changeme
      EMBEDDING_MODEL_NAME: all-MiniLM-L6-v2
      LLAMA_HOST: http://llama:8080
      LLAMA_PROMPT: "A chat between a curious user and an intelligent, polite medical assistant. The assistant provides detailed, helpful answers to the user's medical questions, including accurate references where applicable."
      INDEX_NAME: fasten-index
      UPLOAD_DIR: /app/data/
    ports:
      - "8000:8000"
    volumes:
      - ./data:/app/data
    depends_on:
      elasticsearch:
        condition: service_healthy


  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.12.1
    environment:
      discovery.type: single-node
      xpack.security.enabled: "false"
      xpack.security.http.ssl.enabled: "false"
    ports:
      - "9200:9200"
    mem_limit: 512m
    volumes:
      - esdata:/usr/share/elasticsearch/data
    healthcheck:
      test: ["CMD-SHELL", "curl -fsSL http://localhost:9200/_cat/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3

  llama:
    image: ghcr.io/ggerganov/llama.cpp:server
    volumes:
      - ./model:/models
    command: >
      -m models/Meta-Llama-3-8B-Instruct.Q5_K_M.gguf
      -n -1
      -c 1024
      -t 10
      --host 0.0.0.0
      --port 8080
    ports:
      - "8080:8080"

volumes:
  esdata:
    driver: local
